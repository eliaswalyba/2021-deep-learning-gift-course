{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â 03 - Neural Style Transfer ðŸŽ¨\n",
    "\n",
    "---\n",
    "\n",
    "![](https://images.unsplash.com/photo-1461344577544-4e5dc9487184?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Alice Achterhof](https://unsplash.com/photos/FwF_fKj5tBo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll get a chance to play around with **Neural Style Transfer algorithms**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. What is Neural Style Transfer ?\n",
    "\n",
    "## I.1. Objective\n",
    "\n",
    "These algorithms are specialized for style transfer between:\n",
    "- a base image, called content image, from which we would like to keep the content\n",
    "- a style image from which we would like to take the style and apply it to the content\n",
    "\n",
    "<img src='images/nst.png' width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Intuition\n",
    "\n",
    "The principle of neural style transfer is to **define two distance functions** :\n",
    "- one that describes **how different the content of two images are**, `Lcontent`, \n",
    "- and one that describes the **difference between the two images in terms of their style**, `Lstyle`. \n",
    "\n",
    "Then, given three images :\n",
    "- a **desired style image** (S)\n",
    "- a **desired content image** (C)\n",
    "- and the **generated image** (G) (initialized with the content image)\n",
    "\n",
    "we try to transform the input image to **minimize the content distance with the content image and its style distance with the style image**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Loss computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1. Overall loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:27:48.793005Z",
     "start_time": "2019-06-10T15:27:48.786092Z"
    }
   },
   "source": [
    "The **overall loss** (or total loss) is given by :\n",
    "\n",
    "<img src='images/loss.png' width=\"600px\" />\n",
    "\n",
    "> ðŸ”¦ **Hint**: The coefficients associated to each type of loss are hyper-parameters.\n",
    "\n",
    "During each iteration, all the three images i.e. **content image**, **style image** and **generated image** are passed through the VGG19 model.\n",
    "\n",
    "The value of the hidden unitâ€™s activation which encode feature representation of the given image at certain layers are taken as input to these loss functions.\n",
    "\n",
    "<img src='images/loss_inp.png' width=\"600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2. Content loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **content loss** is simply the L2-loss of the activation layer of the content image vs. the generated image. \n",
    "\n",
    "<img src='images/loss_c.png' width=\"600px\" />\n",
    "\n",
    "> ðŸ”¦**Hint**: We note each activation layer of content image as `a(L)(C)` and activation layer of generated image as `a(L)(G)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:41:05.598464Z",
     "start_time": "2019-06-10T15:41:05.595108Z"
    }
   },
   "source": [
    "## II.3. Style loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **style loss** is more complex and requires to compute the Gram matrix, the loss associated to the Gram matrix between the Style and the Generated image, and the weighted style loss accross layers of the Style and Generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Implementation in Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:15:02.839247Z",
     "start_time": "2019-06-10T15:14:57.318883Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the content image, the style image and the result folder\n",
    "base_image_path = 'drive/My Drive/trump.jpg'\n",
    "style_reference_image_path = 'drive/My Drive/lisa.jpg'\n",
    "result_prefix = 'drive/My Drive/Results_Iterations/'\n",
    "\n",
    "# Pick the number of iterations\n",
    "iterations=100\n",
    "\n",
    "# Weights of the different loss components\n",
    "content_weight=0.025\n",
    "style_weight=1.0\n",
    "total_variation_weight=1.0\n",
    "\n",
    "# Dimensions of the generated picture\n",
    "width, height = load_img(base_image_path).size\n",
    "img_nrows = 400\n",
    "img_ncols = int(width * img_nrows / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, resize and format pictures into appropriate tensors\n",
    "def preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
    "    # Convert to array\n",
    "    img = img_to_array(img)\n",
    "    # Expand dimensions\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    # Use the VGG19 input pre-processing \n",
    "    img = vgg19.preprocess_input(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, img_nrows, img_ncols))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((img_nrows, img_ncols, 3))\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tensor representations of our images\n",
    "base_image = K.variable(preprocess_image(base_image_path))\n",
    "style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n",
    "\n",
    "# This will contain our generated image\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n",
    "else:\n",
    "    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n",
    "\n",
    "# Combine the 3 images into a single Keras tensor\n",
    "input_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VGG19 network with our 3 images as input\n",
    "# The model will be loaded with pre-trained ImageNet weights\n",
    "model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the neural style loss\n",
    "# First we need to define 4 util functions\n",
    "\n",
    "\n",
    "# The gram matrix of an image tensor (feature-wise outer product)\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 3\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        features = K.batch_flatten(x)\n",
    "    else:\n",
    "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "  \n",
    "# The \"style loss\" is designed to maintain the style of the reference image in the generated image.\n",
    "# It is based on the gram matrices (which capture style) of feature maps from the style reference image\n",
    "# and from the generated image\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    assert K.ndim(style) == 3\n",
    "    assert K.ndim(combination) == 3\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_nrows * img_ncols\n",
    "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "  \n",
    "# An auxiliary loss function designed to maintain the \"content\" of the\n",
    "# base image in the generated image\n",
    "\n",
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "  \n",
    "# The 3rd loss function, total variation loss, designed to keep the generated image locally coherent\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    assert K.ndim(x) == 4\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
    "        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
    "    else:\n",
    "        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
    "        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine these loss functions into a single scalar\n",
    "loss = K.variable(0.0)\n",
    "layer_features = outputs_dict['block5_conv2']\n",
    "base_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(base_image_features, combination_features)\n",
    "\n",
    "feature_layers = ['block1_conv1', 'block2_conv1',\n",
    "                  'block3_conv1', 'block4_conv1',\n",
    "                  'block5_conv1']\n",
    "\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradients of the generated image with respect to the loss\n",
    "grads = K.gradients(loss, combination_image)\n",
    "\n",
    "outputs = [loss]\n",
    "if isinstance(grads, (list, tuple)):\n",
    "    outputs += grads\n",
    "else:\n",
    "    outputs.append(grads)\n",
    "\n",
    "f_outputs = K.function([combination_image], outputs)\n",
    "\n",
    "\n",
    "# Evaluate loss and gradients\n",
    "def eval_loss_and_grads(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
    "    else:\n",
    "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    if len(outs[1:]) == 1:\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "    else:\n",
    "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
    "    return loss_value, grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Evaluator class makes it possible to compute loss and gradients in one pass\n",
    "# while retrieving them via two separate functions, \"loss\" and \"grads\". \n",
    "# This is done because scipy.optimize requires separate functions for loss and gradients,\n",
    "# but computing them separately would be inefficient.\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
    "# so as to minimize the neural style loss\n",
    "x = preprocess_image(base_image_path)\n",
    "\n",
    "for i in range(iterations):\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    # save current generated image\n",
    "    img = deprocess_image(x.copy())\n",
    "    if i % 10 == 0 :\n",
    "        print('Start of iteration', i)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "        save_img(fname, img)\n",
    "        end_time = time.time()\n",
    "        print('Current loss value:', min_val)\n",
    "        print('Image saved as', fname)\n",
    "        print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise ðŸŽ“"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now your turn !\n",
    "\n",
    "**Q1**. Use the 2 images given in the image folder of this exercise (`lisa.jpg` and `trump.jpg`) to apply style transfer from Lisa to Trump. Use the code given above. \n",
    "\n",
    "> ðŸ”¦**Hint**: It is highly recommended to do this exercise in Colab using GPUs.\n",
    "\n",
    "**Q2**. Once this is done, use any content and style image you'd like, and try your own style transfer !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRIP_START ###\n",
    "\n",
    "#### Solution\n",
    "Colab Notebook : https://colab.research.google.com/drive/1JM_abH_43FbcGN8LQ82bwXlVpIsXeMZn\n",
    "\n",
    "### STRIP_END ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
